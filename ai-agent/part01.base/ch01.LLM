LLM은 초거대 언어 모델로 언어의 입력과 출력이 가능한 딥러닝 모델.

트랜스포머
- 인코더(언어를 이해) -> 디코더(언어를 생성)
- 기계 번역을 위해서 고안된 아키텍처.

NLP의 역사
NLP, 자연어 처리. 언어 패턴을 통계적으로 분석하고 통계적 모델부터 현대의 멀티모달 모델에 이르기까지 발전.
초기 규칙 기반/통계적 모델 -> 순환 신경망(RNN), 장단기 메모리(LSTM) -> 트랜스포머(어텐션 메커니즘) -> 사전 학습 모델(BERT, GPT 등) -> 거대 멀티모달 모델

BERT는 인코더를 중심으로 개발
GPT는 디코더를 중심으로 개발

트랜스포머의 원리는 무엇인가?
encode와 decode라는 두개의 구조를 갖춘 딥러닝 아키텍처. 주어진 문장을 이해하고, 다음에 올 단어를 예측하는 역할. Attention이라는 매커니즘 중심으로 작동.
Multi-head attention은 문장 내 단어 간의 관계와 문장의 맥락을 다방면으로 이해.

LLM은 어떻게 만들어지는가?
디코더만 쌓아올린 구조로 대량의 언어 데이터-코퍼스로 학습을 시킨 결과 문장을 이해하고 생성하는 능력을 갖추게 되면서 발전한 언어모델.

